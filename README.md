# deeplearningbook


introduction to maching learning
With a few rules, AI is able to tackle simple problems with great accomplishment in early days. Nowadays, we are facing much more complicate.
However subjective and intuitive knowledge invovled tasks remain diffult for AI to rival human. So it's said that "One of the key challenges in
artificial intelligence is how to get this informal knowledge into a computer". The complexity behind various problems required that knowledge are 
to be acquired, instead of hard coded. Machine learning comes to rescue here.

from maching learning to deep learning
Machine learning algorithms relays heavily on the representation of the data, which is known as feature. It can be difficult to extract the proper features
from raw data. Deep learning can solve this problem automatically, this befinits so much to building a powerful AI systems that's capble of coping with  
complicate situations. In my view, it's another useful tool invented to analyse problems, which means you can break hard problems into small and 
easy piceces with it, perform some actions on these parts, and concludes somewhere. 

Overview
This books has been organized into three parts. Part I introduces some fundamental knowledges to understand deep learning. Part II is a practical guideance 
to build a working system.Part III help those researchers and experts in this area out.

History
Deep learning is not a novel idea, it dates back to 1940s. The earliest predecessors of modern deep learning were simple linear models. Linear models has limits.
A few key concepts remaining central to today's deep learning arouse during 1980s, includes distributed representation(use features to represent input), back propagation
to train neatral network.



Part I
Linear Algebra
Scalars,Vectors,Matrices, Tensors
transpose, broadcasting,element-wise product or Hadamard product, dot product,matrix inversion(useful as a theoretical tool, should not be used in practice),

linear combination 
A square matrix(n*n) with linearly dependent columns( independent columns is n*m) is known as singular.
Norms are functions mapping vectors to non-negative values,  used to measure the size of vectors.
Squared L2 norms maybe undesirable because it increases very slowly near the origin. When it's important to discriminate between exactly zero and near zero, L1 norm is recommended.
max norms is the max absolute value of the elments in the vector.

The most common way to measure the matrix size is Frobenius norm, analogous to L2 norm of a vector.
diagonal matrices is computationally effcient in matrix multiplying, and so is its inverse.(In many cases, we may derive some very general machine learning algorithm in terms of arbitrary matrices,but obtain a less expensive (and less descriptive) algorithm by restricting some matrices to be diagonal)
Non-square diagonal matrices do not have inverses but it is still possible to multiply by them cheaply either by concatenating some zeros, or by discarding some elements.

Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments.
the inverse of an orthogonal matrix  is equal to its transpose, that makes orthogonal matrix interesting.
eigen decomposition is one of the most widely used matrix decompision methods, it aims at finding some properties that are universal by breaking matrices into parts. not every matrix can be decomposed into eigenvalues and eigenvectors.
the formal eigen decomposition is composed of three parts: diagnal matrix of eigen values, eigenvectors matrix and its inverse. 
Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:diagnal matrix of eigen values, diageigenvectors matrix and its tranpose(inverse). This means, in the real symmetric matrix case,
the eigen decomposition can be simplified.

SVD is another matrix decomposition, every real matrix has a singular value decomposition. non-square matrix cannot be eigendecomposition, but svd is applicable here.
A(m×n) = U(m×m orthogonal matrices) * D(m×n diagonal matrix)* transpose of V(n×n orthogonal matrices).The elements along the diagonal of D are known as the singular values of the matrix A. the most useful feature of SVD
is to perform matrix inversion to non-square matrix.
the trace operator make it easy to use proper notation to denote complicate operations, invariant to cyclic permutation.
the determinant of a square matrix is a function mapping matrices to real scalars, it's equal to product of all the eigenvalues of matrix.

The PCA is comprehensive in 1d case, however you need 











