# deeplearningbook


introduction to maching learning
With a few rules, AI is able to tackle simple problems with great accomplishment in early days. Nowadays, we are facing much more complicate.
However subjective and intuitive knowledge invovled tasks remain diffult for AI to rival human. So it's said that "One of the key challenges in
artificial intelligence is how to get this informal knowledge into a computer". The complexity behind various problems required that knowledge are 
to be acquired, instead of hard coded. Machine learning comes to rescue here.

from maching learning to deep learning
Machine learning algorithms relays heavily on the representation of the data, which is known as feature. It can be difficult to extract the proper features
from raw data. Deep learning can solve this problem automatically, this befinits so much to building a powerful AI systems that's capble of coping with  
complicate situations. In my view, it's another useful tool invented to analyse problems, which means you can break hard problems into small and 
easy piceces with it, perform some actions on these parts, and concludes somewhere. 

Overview
This books has been organized into three parts. Part I introduces some fundamental knowledges to understand deep learning. Part II is a practical guideance 
to build a working system.Part III help those researchers and experts in this area out.

History
Deep learning is not a novel idea, it dates back to 1940s. The earliest predecessors of modern deep learning were simple linear models. Linear models has limits.
A few key concepts remaining central to today's deep learning arouse during 1980s, includes distributed representation(use features to represent input), back propagation
to train neatral network.



Part I
Linear Algebra
Scalars,Vectors,Matrices, Tensors
transpose, broadcasting,element-wise product or Hadamard product, dot product,matrix inversion(useful as a theoretical tool, should not be used in practice),

linear combination 
A square matrix(n*n) with linearly dependent columns( independent columns is n*m) is known as singular.
Norms are functions mapping vectors to non-negative values,  used to measure the size of vectors.
Squared L2 norms maybe undesirable because it increases very slowly near the origin. When it's important to discriminate between exactly zero and near zero, L1 norm is recommended.
max norms is the max absolute value of the elments in the vector.

The most common way to measure the matrix size is Frobenius norm, analogous to L2 norm of a vector.
diagonal matrices is computationally effcient in matrix multiplying, and so is its inverse.(In many cases, we may derive some very general machine learning algorithm in terms of arbitrary matrices,but obtain a less expensive (and less descriptive) algorithm by restricting some matrices to be diagonal)
Non-square diagonal matrices do not have inverses but it is still possible to multiply by them cheaply either by concatenating some zeros, or by discarding some elements.

Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments.
the inverse of an orthogonal matrix  is equal to its transpose, that makes orthogonal matrix interesting.
eigen decomposition is one of the most widely used matrix decompision methods, it aims at finding some properties that are universal by breaking matrices into parts. not every matrix can be decomposed into eigenvalues and eigenvectors.
the formal eigen decomposition is composed of three parts: diagnal matrix of eigen values, eigenvectors matrix and its inverse. 
Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:diagnal matrix of eigen values, diageigenvectors matrix and its tranpose(inverse). This means, in the real symmetric matrix case,
the eigen decomposition can be simplified.

SVD is another matrix decomposition, every real matrix has a singular value decomposition. non-square matrix cannot be eigendecomposition, but svd is applicable here.
A(m×n) = U(m×m orthogonal matrices) * D(m×n diagonal matrix)* transpose of V(n×n orthogonal matrices).The elements along the diagonal of D are known as the singular values of the matrix A. the most useful feature of SVD
is to perform matrix inversion to non-square matrix.
the trace operator make it easy to use proper notation to denote complicate operations, invariant to cyclic permutation.
the determinant of a square matrix is a function mapping matrices to real scalars, it's equal to product of all the eigenvalues of matrix.

The PCA is comprehensive in 1d case, however you need 






a random variable is just a description of the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are

The probability mass function(PMF) maps from a state of a random variable to the probability of that random variable taking on that state.probability density function (PDF) is used in continuous random variables.

Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution.

In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a conditional probability. 

Computing the consequences of an action is called making an intervention query. conditional probability is not the same as causal modeling.(e.g. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. )

The covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables. High absolute values of the covariance mean that the values change very much and are both far 
from their respective means at the same time. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable 
tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa.
Other measures such as correlation normalize the contribution of each variable in order to measure only how much the variables are related, rather than also being affected by the scale of the separate variables

Independence is a stronger requirement than zero covariance,it is possible for two variables to be dependent but have zero covariance. 

The diagonal elements of the covariance give the variance.

The Bernoulli and multinoulli distributions are sufficient to describe any distribution model with discrete variables for it is feasible to simply enumerate all of the states.

The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution.

the normal distribution is a good default choice In the absence of prior knowledge about what form a distribution over the real numbers should take.

In the context of deep learning, we often want to have a probability distribution with a sharp point at x = 0 . To accomplish this, we can use the exponential distribution.
A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point µ is the Laplace distribution.

In some cases, we wish to specify that all of the mass in a probability distribution clusters around a single point. This can be accomplished by defining a PDF using the Dirac delta function
A common use of the Dirac delta distribution is as a component of an empirical distribution. The Dirac delta distribution is only necessary to define the empirical distribution over continuous variables.For discrete 
variables, the situation is simpler: an empirical distribution can be conceptualized as a multinoulli distribution, with a probability associated to each possible input value that is simply equal to the empirical 
frequency of that value in the training set.


It is also common to define probability distributions by combining other simpler probability distributions.In Chapter 16, we explore the art of building complex probability distributions from simple ones in more detail.
The mixture model allows us to briefly glimpse a concept that will be of paramount importance later—the latent variable. A latent variable is a random variable that we cannot observe directly. 

The logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli distribution because its range is (0,1), which lies within the valid range of values for the φ parameter.

The softplus function can be useful for producing the β or σ parameter of a normal distribution because its range is (0,∞).  The softplus function is intended as a smoothed version of the positive part function, x + = max{0, x}. 
















